services:

  # Main orchestrator server which stiches together all calls to different services to fulfill the user request
  rag-server:
    container_name: rag-server
    image: nvcr.io/nvidia/blueprint/rag-server:${TAG:-2.3.0}
    build:
      # Set context to repo's root directory
      context: ../../
      dockerfile: src/nvidia_rag/rag_server/Dockerfile
    # start the server on port 8081 with 8 workers for improved latency on concurrent requests.
    command: --port 8081 --host 0.0.0.0 --workers 8
    volumes:
      # Mount the prompt.yaml file to the container, path should be absolute
      - ${PROMPT_CONFIG_FILE}:${PROMPT_CONFIG_FILE}
      # Mount host's LanceDB data (used when APP_VECTORSTORE_NAME=lancedb)
      - ${LANCEDB_DATA_PATH:-/tmp/lancedb_data}:/tmp/lancedb_data
    # Common customizations to the pipeline can be controlled using env variables
    environment:
      # Path to example directory relative to root
      EXAMPLE_PATH: './nvidia_rag/rag_server'

      # Absolute path to custom prompt.yaml file
      PROMPT_CONFIG_FILE: ${PROMPT_CONFIG_FILE:-/prompt.yaml}

      ##===MINIO specific configurations which is used to store the multimodal base64 content===
      MINIO_ENDPOINT: "minio:9010"
      MINIO_ACCESSKEY: "minioadmin"
      MINIO_SECRETKEY: "minioadmin"

      ##===Vector DB specific configurations===
      # URL on which vectorstore is hosted
      # For custom operators, point to your service (e.g., http://your-custom-vdb:1234)
      APP_VECTORSTORE_URL: ${APP_VECTORSTORE_URL:-http://milvus:19530}
      # Type of vectordb used to store embedding. Supported built-ins: "milvus", "elasticsearch".
      # You can also provide your custom value (e.g., "your_custom_vdb") when you register it in `_get_vdb_op`.
      APP_VECTORSTORE_NAME: ${APP_VECTORSTORE_NAME:-"milvus"}
      # Type of index to be used for vectorstore
      APP_VECTORSTORE_INDEXTYPE: ${APP_VECTORSTORE_INDEXTYPE:-"GPU_CAGRA"}
      # Type of vectordb search to be used
      APP_VECTORSTORE_SEARCHTYPE: ${APP_VECTORSTORE_SEARCHTYPE:-"dense"} # Can be dense or hybrid
      # Boolean to control GPU search for milvus vectorstore specific to rag-server
      APP_VECTORSTORE_ENABLEGPUSEARCH: ${APP_VECTORSTORE_ENABLEGPUSEARCH:-True}
      # ef: Parameter controlling query time/accuracy trade-off. Higher ef leads to more accurate but slower search.
      APP_VECTORSTORE_EF: ${APP_VECTORSTORE_EF:-100} # Must be greater or equal to VECTOR_DB_TOPK
      # vectorstore collection name to store embeddings
      COLLECTION_NAME: ${COLLECTION_NAME:-multimodal_data}
      APP_RETRIEVER_SCORETHRESHOLD: 0.25
      # Top K from vector DB, which goes as input to reranker model if enabled, else goes to LLM prompt
      VECTOR_DB_TOPK: ${VECTOR_DB_TOPK:-100}

      ##===LLM Model specific configurations===
      APP_LLM_MODELNAME: ${APP_LLM_MODELNAME:-"nvidia/llama-3.3-nemotron-super-49b-v1.5"}
      # url on which llm model is hosted. If "", Nvidia hosted API is used
      APP_LLM_SERVERURL: ${APP_LLM_SERVERURL-"nim-llm:8000"}
      # LLM model parameters
      LLM_MAX_TOKENS: ${LLM_MAX_TOKENS:-32768}
      LLM_TEMPERATURE: ${LLM_TEMPERATURE:-0}
      LLM_TOP_P: ${LLM_TOP_P:-1.0}

      ##===Query Rewriter Model specific configurations===
      APP_QUERYREWRITER_MODELNAME: ${APP_QUERYREWRITER_MODELNAME:-"nvidia/llama-3.3-nemotron-super-49b-v1.5"}
      # url on which query rewriter model is hosted. If "", Nvidia hosted API is used
      APP_QUERYREWRITER_SERVERURL: ${APP_QUERYREWRITER_SERVERURL-"nim-llm:8000"}

      ##===Filter Expression Generator Model specific configurations===
      APP_FILTEREXPRESSIONGENERATOR_MODELNAME: ${APP_FILTEREXPRESSIONGENERATOR_MODELNAME:-"nvidia/llama-3.3-nemotron-super-49b-v1.5"}
      # url on which filter expression generator model is hosted. If "", Nvidia hosted API is used
      APP_FILTEREXPRESSIONGENERATOR_SERVERURL: ${APP_FILTEREXPRESSIONGENERATOR_SERVERURL-"nim-llm:8000"}
      # enable filter expression generator for natural language to filter expression conversion
      ENABLE_FILTER_GENERATOR: ${ENABLE_FILTER_GENERATOR:-False}

      ##===Embedding Model specific configurations===
      # url on which embedding model is hosted. If "", Nvidia hosted API is used
      APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-embedding-ms:8000"}
      APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nv-embedqa-1b-v2}
      # For VLM Embedding Model (Nemoretriever-1b-vlm-embed-v1)
      # APP_EMBEDDINGS_SERVERURL: ${APP_EMBEDDINGS_SERVERURL-"nemoretriever-vlm-embedding-ms:8000"}
      # APP_EMBEDDINGS_MODELNAME: ${APP_EMBEDDINGS_MODELNAME:-nvidia/llama-3.2-nemoretriever-1b-vlm-embed-v1}

      ##===Reranking Model specific configurations===
      # url on which ranking model is hosted. If "", Nvidia hosted API is used
      APP_RANKING_SERVERURL: ${APP_RANKING_SERVERURL-"nemoretriever-ranking-ms:8000"}
      APP_RANKING_MODELNAME: ${APP_RANKING_MODELNAME:-"nvidia/llama-3.2-nv-rerankqa-1b-v2"}
      ENABLE_RERANKER: ${ENABLE_RERANKER:-True}
      # Default confidence threshold for filtering documents by reranker relevance scores (0.0 to 1.0)
      RERANKER_CONFIDENCE_THRESHOLD: ${RERANKER_CONFIDENCE_THRESHOLD:-0.0}

      ##===VLM Model specific configurations===
      ENABLE_VLM_INFERENCE: ${ENABLE_VLM_INFERENCE:-false}
      # Reasoning gate on VLM response: off by default; enable to mitigate incorrect VLM outputs
      ENABLE_VLM_RESPONSE_REASONING: ${ENABLE_VLM_RESPONSE_REASONING:-false}
      # Max images sent to VLM per request (query + context)
      APP_VLM_MAX_TOTAL_IMAGES: ${APP_VLM_MAX_TOTAL_IMAGES:-4}
      # Max number of query images to include in VLM input
      APP_VLM_MAX_QUERY_IMAGES: ${APP_VLM_MAX_QUERY_IMAGES:-1}
      # Max number of context images to include in VLM input
      APP_VLM_MAX_CONTEXT_IMAGES: ${APP_VLM_MAX_CONTEXT_IMAGES:-1}
      # Use VLM only for final response generation
      APP_VLM_RESPONSE_AS_FINAL_ANSWER: ${APP_VLM_RESPONSE_AS_FINAL_ANSWER:-false}
      # VLM server URL
      APP_VLM_SERVERURL: ${APP_VLM_SERVERURL-"http://vlm-ms:8000/v1"}
      # VLM model name
      APP_VLM_MODELNAME: ${APP_VLM_MODELNAME:-"nvidia/llama-3.1-nemotron-nano-vl-8b-v1"}

      NVIDIA_API_KEY: ${NGC_API_KEY:?"NGC_API_KEY is required"}

      # Set LLM_API_KEY if non-NVIDIA hosted LLM API endpoint is being used
      # If not set, NVIDIA_API_KEY will be used as fallback
      LLM_API_KEY: ${LLM_API_KEY:-""}

      # Number of document chunks to insert in LLM prompt, used only when ENABLE_RERANKER is set to True
      APP_RETRIEVER_TOPK: ${APP_RETRIEVER_TOPK:-10}

      # Log level for server, supported level NOTSET, DEBUG, INFO, WARN, ERROR, CRITICAL
      LOGLEVEL: ${LOGLEVEL:-INFO}

      # enable query rewriting for multiturn conversation in the rag chain.
      # This will improve accuracy of the retrieiver pipeline but increase latency due to an additional LLM call
      ENABLE_QUERYREWRITER: ${ENABLE_QUERYREWRITER:-False}

      # Choose whether to enable citations in the response
      ENABLE_CITATIONS: ${ENABLE_CITATIONS:-True}

      # Choose whether to enable/disable guardrails
      ENABLE_GUARDRAILS: ${ENABLE_GUARDRAILS:-False}

      # NeMo Guardrails URL when ENABLE_GUARDRAILS is true
      NEMO_GUARDRAILS_URL: ${NEMO_GUARDRAILS_URL:-nemo-guardrails-microservice:7331}

      # number of last n chat messages to consider from the provided conversation history
      CONVERSATION_HISTORY: 5

      # Tracing
      APP_TRACING_ENABLED: ${APP_TRACING_ENABLED:-"False"}
      # HTTP endpoint
      APP_TRACING_OTLPHTTPENDPOINT: http://otel-collector:4318/v1/traces
      # GRPC endpoint
      APP_TRACING_OTLPGRPCENDPOINT: grpc://otel-collector:4317
      # Prometheus multi-process metrics directory
      PROMETHEUS_MULTIPROC_DIR: "/tmp-data/prom_data"

      # Choose whether to enable source metadata in document content during generation
      ENABLE_SOURCE_METADATA: ${ENABLE_SOURCE_METADATA:-true}

      # Whether to filter content within <think></think> tags in model responses
      FILTER_THINK_TOKENS: ${FILTER_THINK_TOKENS:-true}

      # enable reflection (context relevance and response groundedness checking) in the rag chain
      ENABLE_REFLECTION: ${ENABLE_REFLECTION:-false}
      # Maximum number of context relevance loop iterations
      MAX_REFLECTION_LOOP: ${MAX_REFLECTION_LOOP:-3}
      # Minimum relevance score threshold (0-2)
      CONTEXT_RELEVANCE_THRESHOLD: ${CONTEXT_RELEVANCE_THRESHOLD:-1}
      # Minimum groundedness score threshold (0-2)
      RESPONSE_GROUNDEDNESS_THRESHOLD: ${RESPONSE_GROUNDEDNESS_THRESHOLD:-1}
      # reflection llm
      REFLECTION_LLM: ${REFLECTION_LLM:-"nvidia/llama-3.3-nemotron-super-49b-v1.5"}
      # reflection llm server url. If "", Nvidia hosted API is used
      REFLECTION_LLM_SERVERURL: ${REFLECTION_LLM_SERVERURL-"nim-llm:8000"}
      # enable iterative query decomposition
      ENABLE_QUERY_DECOMPOSITION: ${ENABLE_QUERY_DECOMPOSITION:-false}
      # maximum recursion depth for iterative query decomposition
      MAX_RECURSION_DEPTH: ${MAX_RECURSION_DEPTH:-3}

    ports:
      - "8081:8081"
    expose:
      - "8081"
    shm_size: 5gb

  # Sample UI container which interacts with APIs exposed by rag-server container
  rag-frontend:
    container_name: rag-frontend
    image: nvcr.io/nvidia/blueprint/rag-frontend:${TAG:-2.3.0}
    build:
      # Set context to repo's root directory
      context: ../../frontend
      dockerfile: ./Dockerfile
      args:
        # Environment variables for Vite build
        VITE_API_CHAT_URL: ${VITE_API_CHAT_URL:-http://rag-server:8081/v1}
        VITE_API_VDB_URL: ${VITE_API_VDB_URL:-http://ingestor-server:8082/v1}
        VITE_MILVUS_URL: http://milvus:19530
    ports:
      - "8090:3000"
    expose:
      - "3000"
    environment:
      # Runtime environment variables for Vite
      VITE_API_CHAT_URL: ${VITE_API_CHAT_URL:-http://rag-server:8081/v1}
      VITE_API_VDB_URL: ${VITE_API_VDB_URL:-http://ingestor-server:8082/v1}
      VITE_MILVUS_URL: http://milvus:19530
    depends_on:
      - rag-server

networks:
  default:
    name: nvidia-rag
